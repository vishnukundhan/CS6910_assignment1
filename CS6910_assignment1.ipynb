{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_IR-mprtZf9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection  import train_test_split\n",
        "import itertools\n",
        "import math\n",
        "%matplotlib inline\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tw_B3DrtcgE"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1 : Printing sample images from each class"
      ],
      "metadata": {
        "id": "E0VbR4zaQ_8x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVHtIRgktequ"
      },
      "outputs": [],
      "source": [
        "wandb.init(project='CS6910-assignment1')\n",
        "samples = list(y_train)\n",
        "class_names = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "for i in range(10):\n",
        "    j=list.index(samples,i)\n",
        "    #print(j)\n",
        "    wandb.log({'label': i, 'image': [wandb.Image(x_train[j], caption=class_names[i])]})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val  = train_test_split(x_train, y_train, test_size=0.1,random_state = 42)"
      ],
      "metadata": {
        "id": "H-PA6_gE0NPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = x_train.reshape(x_train.shape[0],-1)\n",
        "center = tmp - tmp.mean(axis=0)\n",
        "x_train = center/np.max(center.max(axis=0))\n",
        "\n",
        "tmp = x_val.reshape(x_val.shape[0],-1)\n",
        "center = tmp - tmp.mean(axis=0)\n",
        "x_val = center/np.max(center.max(axis=0))\n",
        "\n",
        "tmp = x_test.reshape(x_test.shape[0],-1)\n",
        "center = tmp - tmp.mean(axis=0)\n",
        "x_test = center/np.max(center.max(axis=0))"
      ],
      "metadata": {
        "id": "s6ShPDk24y1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2, Q3 : y_hat gives the probability distribution over 10 classes. All the functions were written inside the class \"NeualNetwork\""
      ],
      "metadata": {
        "id": "L0NyK_KARL-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, sizeOfInput, numberOfNeuronsEachLayer, numberOfLayers, activationFunction, typeOfInit, L2regConst = 0):\n",
        "        self.activationFunction = activationFunction\n",
        "        self.sizeOfInput = sizeOfInput\n",
        "        self.numberOfLayers = numberOfLayers\n",
        "        self.numberOfNeuronsEachLayer = numberOfNeuronsEachLayer\n",
        "        self.L2regConst = L2regConst\n",
        "        self.W, self.b = self.initializer(typeOfInit)\n",
        "\n",
        "    #Initialize weights and biases\n",
        "    def initializer(self, init):\n",
        "        W = [], b = []\n",
        "        if init == 'random':\n",
        "            W.append(np.random.randn(self.numberOfNeuronsEachLayer[0], self.sizeOfInput))\n",
        "            for i in range(1,self.numberOfLayers):\n",
        "                W.append(np.random.randn(self.numberOfNeuronsEachLayer[i],self.numberOfNeuronsEachLayer[i-1]))\n",
        "            for i in range(self.numberOfLayers):\n",
        "                b.append(np.random.rand(self.numberOfNeuronsEachLayer[i]))\n",
        "\n",
        "        elif(init == 'xavier'):\n",
        "            W.append(np.random.normal(0,math.sqrt(2/(self.numberOfNeuronsEachLayer[0]+ self.sizeOfInput)), (self.numberOfNeuronsEachLayer[0], self.sizeOfInput)))\n",
        "            for i in range(1,self.numberOfLayers):\n",
        "                W.append(np.random.normal(0, math.sqrt(2/(self.numberOfNeuronsEachLayer[i]+self.numberOfNeuronsEachLayer[i-1])),(self.numberOfNeuronsEachLayer[i],self.numberOfNeuronsEachLayer[i-1])))\n",
        "            for i in range(self.numberOfLayers):\n",
        "                b.append(np.random.rand(self.numberOfNeuronsEachLayer[i]))\n",
        "        return W,b\n",
        "\n",
        "    def initialize(self, sizeOfInput, numberOfLayers, numberOfNeuronsEachLayer):\n",
        "        W, b = [], []\n",
        "        W.append(np.zeros((numberOfNeuronsEachLayer[0], sizeOfInput)))\n",
        "        b.append(np.zeros(numberOfNeuronsEachLayer[0]))\n",
        "        for i in range(1,numberOfLayers):\n",
        "            W.append(np.zeros((numberOfNeuronsEachLayer[i],numberOfNeuronsEachLayer[i-1])))\n",
        "            b.append(np.zeros(numberOfNeuronsEachLayer[i]))\n",
        "        return W, b\n",
        "\n",
        "    #Function calls for all optimizers\n",
        "    def optimize(self, X, Y, valImages, valLabels, optimizer, learningRate, epochs, batchSize):\n",
        "        if optimizer == 'sgd':\n",
        "            self.stochastic_gradient_descent(X, Y, valImages, valLabels, learningRate, epochs)\n",
        "        elif optimizer == 'momentum':\n",
        "            self.momentum_gradient_descent(X, Y, valImages,valLabels, learningRate, epochs, batchSize)\n",
        "        elif optimizer == 'nag':\n",
        "            self.nesterov_accelerated_gradient_descent(X, Y, valImages, valLabels, learningRate, epochs, batchSize)\n",
        "        elif optimizer == 'rmsprop':\n",
        "            self.rmsprop(X, Y, valImages,valLabels, learningRate, epochs, batchSize)\n",
        "        elif optimizer == 'adam':\n",
        "            self.adam(X, Y, valImages,valLabels, learningRate, epochs, batchSize)\n",
        "        elif optimizer == 'nadam':\n",
        "            self.nadam(X, Y, valImages, valLabels, learningRate, epochs, batchSize)\n",
        "\n",
        "    def activation(self,x):\n",
        "        if self.activationFunction == 'relu':\n",
        "            return self.ReLU(x)\n",
        "        elif self.activationFunction == 'tanh':\n",
        "            return self.tanh(x)\n",
        "        elif self.activationFunction == 'sigmoid':\n",
        "            return self.sigmoid(x)\n",
        "\n",
        "    def activationDerivative(self,x):\n",
        "        if self.activationFunction == 'relu':\n",
        "            return self.ReLUDerivative(x)\n",
        "        elif self.activationFunction == 'tanh':\n",
        "            return self.tanhDerivative(x)\n",
        "        elif self.activationFunction == 'sigmoid':\n",
        "            return self.sigmoidDerivative(x)\n",
        "\n",
        "    def ReLU(self,Z):\n",
        "        return np.maximum(0,Z)\n",
        "\n",
        "    def ReLUDerivative(self,Z):\n",
        "        return [1 if x>0 else 0 for x in Z]\n",
        "\n",
        "    def tanh(self, Z):\n",
        "        return np.array([((np.exp(x) - np.exp(-x))/((np.exp(x) + np.exp(-x)))) for x in Z])\n",
        "\n",
        "    def tanhDerivative(self, Z):\n",
        "        return np.array(1 - self.tanh(Z)**2)\n",
        "\n",
        "    def sigmoidDerivative(self,Z):\n",
        "        return self.sigmoid(Z)*(1-self.sigmoid(Z))\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return np.where(x>=0, 1/(1+np.exp(-x)), np.exp(x)/(1+np.exp(x)))\n",
        "\n",
        "    def softmaxFunction(self,Z):\n",
        "        Z = Z - Z.max()\n",
        "        return (np.exp(Z)/np.sum(np.exp(Z),axis=0))\n",
        "\n",
        "    #Forward and backward Propagation\n",
        "    def forwardPropagation(self,Input):\n",
        "        A = []\n",
        "        H = []\n",
        "        Input = np.array(Input)\n",
        "        A.append(self.W[0].dot(Input) + self.b[0])\n",
        "        for i in range(1, self.numberOfLayers):\n",
        "            H.append(self.activation(A[-1]))\n",
        "            A.append(self.W[i].dot(H[-1]) + self.b[i])\n",
        "        y_hat = self.softmaxFunction(A[-1])\n",
        "        return A, H, y_hat\n",
        "\n",
        "    def backwardPropagation(self, A, H, y_hat, y, Input):\n",
        "        delA = []\n",
        "        delH = []\n",
        "        delW = []\n",
        "        delb = []\n",
        "        Input = np.array(Input)\n",
        "        H.insert(0,Input)\n",
        "        ey = np.zeros(self.numberOfNeuronsEachLayer[-1])\n",
        "        ey[y] = 1\n",
        "        delA.append(np.array(-(ey - y_hat)))\n",
        "        for i in range(self.numberOfLayers-1,-1,-1):\n",
        "            delW.insert(0,delA[-1].reshape(delA[-1].shape[0],1).dot(H[i].reshape(H[i].shape[0],1).T) + self.L2regConst*self.W[i])\n",
        "            delb.insert(0,delA[-1])\n",
        "            delH.append(self.W[i].T.dot(delA[-1]))\n",
        "            if i-1>=0:\n",
        "                delA.append(np.multiply(delH[-1], self.activationDerivative(A[i-1])))\n",
        "        return delW,delb\n",
        "\n",
        "    #Validation loss and validation accuracy\n",
        "    def valLossAccuracy(self,valImages,valLabels):\n",
        "        count = 0\n",
        "        error = 0\n",
        "        for i in range(valImages.shape[0]):\n",
        "            A,H,y_hat = self.forwardPropagation(valImages[i])\n",
        "            total = [x.sum() for x in self.W]\n",
        "            error += -math.log(y_hat[valLabels[i]]) + self.L2regConst/2*sum(total)\n",
        "            if np.argmax(y_hat) == valLabels[i]:\n",
        "                count += 1\n",
        "        return error/valImages.shape[0], count/valImages.shape[0]*100\n",
        "\n",
        "    #Test result and accuracy\n",
        "    def test(self, testImages, testLabels):\n",
        "        count = 0\n",
        "        y_hat = []\n",
        "        for i in range(testImages.shape[0]):\n",
        "            A,H,y = self.forwardPropagation(testImages[i])\n",
        "            if np.argmax(y) == testLabels[i]:\n",
        "                count += 1\n",
        "            y_hat.append(y)\n",
        "        return np.argmax(np.array(y_hat),axis=1), count/testImages.shape[0]*100\n",
        "\n",
        "    #All optimisation functions are coded below\n",
        "\n",
        "    def stochastic_gradient_descent(self, X, Y, valImages, valLabels, learningRate, epochs):\n",
        "        for j in range(epochs):\n",
        "            correct = 0\n",
        "            error = 0\n",
        "            delW, delb = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "            for i in range(X.shape[0]):\n",
        "                A,H,y_hat = self.forwardPropagation(X[i])\n",
        "                s = [x.sum() for x in self.W]\n",
        "                error += -math.log(y_hat[Y[i]]) + self.L2regConst/2*sum(s)\n",
        "                delW,delb = self.backwardPropagation(A,H,y_hat,Y[i],X[i])\n",
        "                if(np.argmax(y_hat) == Y[i]):\n",
        "                    correct +=1\n",
        "                for i in range(self.numberOfLayers):\n",
        "                    self.W[i] = self.W[i] - learningRate*delW[i]\n",
        "                    self.b[i] = self.b[i] - learningRate*delb[i]\n",
        "\n",
        "            error /= X.shape[0]\n",
        "            accuracy = correct/X.shape[0]*100\n",
        "            validLoss, validAccuracy = self.valLossAccuracy(valImages, valLabels)\n",
        "            #print('epoch', j+1, 'loss', error, 'accuracy', accuracy, 'valid_loss', validLoss, 'valid_accuracy', validAccuracy)\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : validLoss,'valid_accuracy' : validAccuracy})\n",
        "\n",
        "    def nesterov_accelerated_gradient_descent(self, X, Y, valImages, valLabels, learningRate, epochs, batchSize, gamma = 0.5):\n",
        "        updateW, updateb = self.initialize( self.sizeOfInput, self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "        lookaheadW, lookaheadb = self.initialize( self.sizeOfInput, self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "        thetaW, thetab = self.initialize( self.sizeOfInput, self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            count = 0\n",
        "            error = 0\n",
        "            delW, delb = self.initialize( self.sizeOfInput, self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "            for k in range( self.numberOfLayers):\n",
        "                thetaW[k] = self.W[k]\n",
        "                thetab[k] = self.b[k]\n",
        "                lookaheadW[k] = thetaW[k] - gamma*updateW[k]\n",
        "                lookaheadb[k] = thetab[k] - gamma*updateb[k]\n",
        "                self.W[k] = lookaheadW[k]\n",
        "                self.b[k] = lookaheadb[k]\n",
        "\n",
        "            for i in range(X.shape[0]):\n",
        "                A,H,y_hat = self.forwardPropagation(X[i])\n",
        "                s = [x.sum() for x in self.W]\n",
        "                error += -math.log(y_hat[Y[i]]) + self.L2regConst/2*sum(s)\n",
        "                w,b = self.backwardPropagation(A,H,y_hat,Y[i],X[i])\n",
        "\n",
        "                for k in range( self.numberOfLayers):\n",
        "                    delW[k] += w[k]\n",
        "                    delb[k] += b[k]\n",
        "                    updateW[k] = gamma*updateW[k] + learningRate*delW[k]\n",
        "                    updateb[k] = gamma*updateb[k] + learningRate*delb[k]\n",
        "\n",
        "                if  (i%batchSize == 0 and i!=0) or i==X.shape[0]-1:\n",
        "                    delW, delb = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "                    for k in range(self.numberOfLayers):\n",
        "                        self.W[k] += -updateW[k]\n",
        "                        self.b[k] += -updateb[k]\n",
        "\n",
        "                if(np.argmax(y_hat) == Y[i]):\n",
        "                    count +=1\n",
        "\n",
        "            error /= X.shape[0]\n",
        "            accuracy = count/X.shape[0]*100\n",
        "            validLoss, validAccuracy = self.valLossAccuracy(valImages, valLabels)\n",
        "            #print('epoch', j+1, 'loss', error, 'accuracy', accuracy, 'valid_loss', validLoss, 'valid_accuracy', validAccuracy)\n",
        "            wandb.log({'epoch' : j+1, 'loss' : error, 'accuracy' : accuracy,'val_loss' : validLoss,'val_accuracy' : validAccuracy})\n",
        "\n",
        "    def momentum_gradient_descent(self,X, Y, valImages, valLabels, learningRate, epochs, batchSize, gamma = 0.6):\n",
        "        updateW, updateb = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            count = 0\n",
        "            error = 0\n",
        "            delW, delb = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "            for i in range(X.shape[0]):\n",
        "                A,H,y_hat = self.forwardPropagation(X[i])\n",
        "                s = [x.sum() for x in self.W]\n",
        "                error += -math.log(y_hat[Y[i]]) + self.L2regConst/2*sum(s)\n",
        "                w,b = self.backwardPropagation(A,H,y_hat,Y[i],X[i])\n",
        "\n",
        "                for k in range(self.numberOfLayers):\n",
        "                    delW[k] += w[k]\n",
        "                    delb[k] += b[k]\n",
        "                    updateW[k] = gamma*updateW[k] + learningRate*delW[k]\n",
        "                    updateb[k] = gamma*updateb[k] + learningRate*delb[k]\n",
        "\n",
        "                if  (i%batchSize == 0 and i!=0) or i==X.shape[0]-1:\n",
        "                    delW, delb = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "                    for k in range(self.numberOfLayers):\n",
        "                        self.W[k] += -updateW[k]\n",
        "                        self.b[k] += -updateb[k]\n",
        "\n",
        "                if(np.argmax(y_hat) == Y[i]):\n",
        "                    count +=1\n",
        "\n",
        "            error /= X.shape[0]\n",
        "            accuracy = count/X.shape[0]*100\n",
        "            validLoss, validAccuracy = self.valLossAccuracy(valImages, valLabels)\n",
        "            #print('epoch', j+1, 'loss', error, 'accuracy', accuracy, 'valid_loss', validLoss, 'valid_accuracy', validAccuracy)\n",
        "            wandb.log({'epoch' : j+1, 'loss' : error, 'accuracy' : accuracy, 'val_loss' : validLoss, 'val_accuracy' : validAccuracy})\n",
        "\n",
        "    def rmsprop(self,X, Y, valImages,valLabels, learningRate, epochs, batchSize, beta = 0.89, epsilon = 1e-6):\n",
        "        v_W, v_b = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            count = 0\n",
        "            error = 0\n",
        "            delW, delb = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "            for i in range(X.shape[0]):\n",
        "                A,H,y_hat = self.forwardPropagation(X[i])\n",
        "                s = [x.sum() for x in self.W]\n",
        "                error += -math.log(y_hat[Y[i]]) + self.L2regConst/2*sum(s)\n",
        "                w,b = self.backwardPropagation(A,H,y_hat,Y[i],X[i])\n",
        "\n",
        "                for k in range(self.numberOfLayers):\n",
        "                    delW[k] += w[k]\n",
        "                    delb[k] += b[k]\n",
        "                    v_W[k] =  beta*v_W[k] + (1-beta)*delW[k]**2\n",
        "                    v_b[k] = beta*v_b[k] + (1-beta)*delb[k]**2\n",
        "\n",
        "                if(np.argmax(y_hat) == Y[i]):\n",
        "                    count +=1\n",
        "\n",
        "                if  (i%batchSize == 0 and i!=0) or i==X.shape[0]-1:\n",
        "                    for k in range(self.numberOfLayers):\n",
        "                        v_W[k] =  beta*v_W[k] + (1-beta)*delW[k]**2\n",
        "                        v_b[k] = beta*v_b[k] + (1-beta)*delb[k]**2\n",
        "                        self.W[k] = self.W[k] - (learningRate*delW[k])/np.sqrt(v_W[k] + epsilon)\n",
        "                        self.b[k] = self.b[k] - (learningRate*delb[k])/np.sqrt(v_b[k] + epsilon)\n",
        "                    delW, delb = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "            error /= X.shape[0]\n",
        "            accuracy = count/X.shape[0]*100\n",
        "            validLoss, validAccuracy = self.valLossAccuracy(valImages, valLabels)\n",
        "            #print('epoch', j+1, 'loss', error, 'accuracy', accuracy, 'valid_loss', validLoss, 'valid_accuracy', validAccuracy)\n",
        "            wandb.log({'epoch' : j+1, 'loss' : error, 'accuracy' : accuracy,'val_loss' : validLoss,'val_accuracy' : validAccuracy})\n",
        "\n",
        "    def adam(self, X, Y, valImages, valLabels, learningRate, epochs, batchSize, beta1 = 0.89, beta2 = 0.989, epsilon = 1e-8):\n",
        "        m_W, m_b = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "        m_hat_W, m_hat_b = self.initialize( self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "        v_W, v_b = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "        v_hat_W, v_hat_b = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            count = 0\n",
        "            error = 0\n",
        "            delW, delb = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "            for i in range(X.shape[0]):\n",
        "                A,H,y_hat = self.forwardPropagation(X[i])\n",
        "                s = [x.sum() for x in self.W]\n",
        "                error += -math.log(y_hat[Y[i]]) + self.L2regConst/2*sum(s)\n",
        "                w,b = self.backwardPropagation(A,H,y_hat,Y[i],X[i])\n",
        "\n",
        "                for k in range(self.numberOfLayers):\n",
        "                    delW[k] += w[k]\n",
        "                    delb[k] += b[k]\n",
        "\n",
        "                if(np.argmax(y_hat) == Y[i]):\n",
        "                    count +=1\n",
        "\n",
        "                if  (i%batchSize == 0 and i!=0) or i==X.shape[0]-1:\n",
        "                    for k in range(self.numberOfLayers):\n",
        "                        v_W[k] =  beta2*v_W[k] + (1-beta2)*delW[k]*delW[k]\n",
        "                        v_b[k] = beta2*v_b[k] + (1-beta2)*delb[k]*delb[k]\n",
        "                        m_W[k] = beta1*m_W[k] + (1-beta1)*delW[k]\n",
        "                        m_b[k] = beta1*m_b[k] + (1-beta1)*delb[k]\n",
        "                        m_hat_W[k] = m_W[k]/(math.pow(beta1,j))\n",
        "                        m_hat_b[k] = m_b[k]/(math.pow(beta1,j))\n",
        "                        v_hat_W[k] = v_W[k]/(math.pow(beta2,j))\n",
        "                        v_hat_b[k] = v_b[k]/(math.pow(beta2,j))\n",
        "                        self.W[k] = self.W[k] - (learningRate*m_hat_W[k])/np.sqrt(v_hat_W[k] + epsilon)\n",
        "                        self.b[k] = self.b[k] - (learningRate*m_hat_b[k])/np.sqrt(v_hat_b[k] + epsilon)\n",
        "                    delW, delb = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "            error /= X.shape[0]\n",
        "            accuracy = count/X.shape[0]*100\n",
        "            validLoss, validAccuracy = self.valLossAccuracy(valImages, valLabels)\n",
        "            #print('epoch', j+1, 'loss', error, 'accuracy', accuracy, 'valid_loss', validLoss, 'valid_accuracy', validAccuracy)\n",
        "            wandb.log({'epoch' : j+1, 'loss' : error, 'accuracy' : accuracy,'val_loss' : validLoss,'val_accuracy' : validAccuracy})\n",
        "\n",
        "    def nadam(self, X, Y, valImages, valLabels, learningRate, epochs, batchSize, beta1 = 0.89, beta2 = 0.989, epsilon = 1e-8):\n",
        "        m_W, m_b = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "        m_hat_W, m_hat_b = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "        v_W, v_b = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "        v_hat_W, v_hat_b = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "        for j in range(epochs):\n",
        "            count = 0\n",
        "            error = 0\n",
        "            delW, delb = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "\n",
        "            for i in range(X.shape[0]):\n",
        "                A,H,y_hat = self.forwardPropagation(X[i])\n",
        "                s = [x.sum() for x in self.W]\n",
        "                error += -math.log(y_hat[Y[i]]) + self.L2regConst/2*sum(s)\n",
        "                w,b = self.backwardPropagation(A,H,y_hat,Y[i],X[i])\n",
        "\n",
        "                for k in range(self.numberOfLayers):\n",
        "                    delW[k] += w[k]\n",
        "                    delb[k] += b[k]\n",
        "\n",
        "                if(np.argmax(y_hat) == Y[i]):\n",
        "                    count +=1\n",
        "\n",
        "                if  (i%batchSize == 0 and i!=0) or i==X.shape[0]-1:\n",
        "                    for k in range(self.numberOfLayers):\n",
        "                        v_W[k] =  beta2*v_W[k] + (1-beta2)*delW[k]**2\n",
        "                        v_b[k] = beta2*v_b[k] + (1-beta2)*delb[k]**2\n",
        "                        m_W[k] = beta1*m_W[k] + (1-beta1)*delW[k]\n",
        "                        m_b[k] = beta1*m_b[k] + (1-beta1)*delb[k]\n",
        "                        m_hat_W[k] = m_W[k]/(math.pow(beta1,j))\n",
        "                        m_hat_b[k] = m_b[k]/(math.pow(beta1,j))\n",
        "                        v_hat_W[k] = v_W[k]/(math.pow(beta2,j))\n",
        "                        v_hat_b[k] = v_b[k]/(math.pow(beta2,j))\n",
        "                        self.W[k] = self.W[k] - (learningRate*(beta1*m_hat_W[k] + (1-beta1)*delW[k]/(1-beta1)))/np.sqrt(v_hat_W[k] + epsilon)\n",
        "                        self.b[k] = self.b[k] - (learningRate*(beta1*m_hat_b[k] + (1-beta1)*delb[k]/(1-beta1)))/np.sqrt(v_hat_b[k] + epsilon)\n",
        "                    delW, delb = self.initialize(self.sizeOfInput,self.numberOfLayers,self.numberOfNeuronsEachLayer)\n",
        "            error /= X.shape[0]\n",
        "            accuracy = count/X.shape[0]*100\n",
        "            validLoss, validAccuracy = self.valLossAccuracy(valImages, valLabels)\n",
        "            #print('epoch', j+1, 'loss', error, 'accuracy', accuracy, 'valid_loss', validLoss, 'valid_accuracy', validAccuracy)\n",
        "            wandb.log({'epoch' : j+1, 'loss' : error, 'accuracy' : accuracy,'val_loss' : validLoss,'val_accuracy' : validAccuracy})\n",
        "\n"
      ],
      "metadata": {
        "id": "vZYagVCl98NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4, Q5, Q6 : Sweep using bayesian. I ran 271 sweeps and displayed the plots in the report"
      ],
      "metadata": {
        "id": "ykbOvsMp2hYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sweep\n",
        "sweep_config = {\n",
        "    'name':\"my-sweep\",\n",
        "    'method': 'bayes',\n",
        "    'metric': {'goal': 'maximize', 'name': 'valid_accuracy'},\n",
        "    'parameters':{\n",
        "                    'activation': {\n",
        "                        'values': ['sigmoid', 'tanh', 'relu']\n",
        "                    },\n",
        "                    'batch_size': {\n",
        "                        'values': [16, 32, 64]\n",
        "                    },\n",
        "                    'epochs': {\n",
        "                        'values': [5,10]\n",
        "                    },\n",
        "                    'hidden_inputsize': {\n",
        "                        'values': [32,64,128]\n",
        "                    },\n",
        "                    'number_hidden': {\n",
        "                        'values' : [3,4,5]\n",
        "                    },\n",
        "                    'learning_rate': {\n",
        "                        'values': [1e-3, 1e-4]\n",
        "                    },\n",
        "                    'optimizer': {\n",
        "                        'values': ['sgd', 'momentum', 'nag', 'rmsprop', 'adam', 'nadam']\n",
        "                    },\n",
        "                    'weight_decay': {\n",
        "                        'values': [0, 0.0005, 0.5]\n",
        "                    },\n",
        "                    'weight_init': {\n",
        "                        'values': ['random', 'xavier']\n",
        "                    }\n",
        "                 }\n",
        "    }\n",
        "sweep_id = wandb.sweep(sweep_config, project='CS6910-assignment1')\n"
      ],
      "metadata": {
        "id": "otHBwjScl8Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    var1 = wandb.init()\n",
        "    var2 = var1.config\n",
        "    wandb.run.name = \"hl_\" + str(var2.hidden_inputsize)+\"_bs_\"+str(var2.batch_size)+\"_ac_\"+ var2.activation\n",
        "    np.random.seed(1)\n",
        "    obj = NeuralNetwork(x_train.shape[1], list(itertools.chain(*[[var2.hidden_inputsize]*var2.number_hidden, [10]])), var2.number_hidden+1, var2.activation, var2.weight_init, var2.weight_decay)\n",
        "    obj.optimize(x_train, y_train, x_val, y_val, var2.optimizer, var2.learning_rate, var2.epochs, var2.batch_size)\n",
        "\n",
        "wandb.agent(sweep_id, train)\n"
      ],
      "metadata": {
        "id": "FE2rrZmFIo_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7 : Best accuracy is possible for the configuration shown below.\n",
        "     Generating y prediction to plot the confusion matrix."
      ],
      "metadata": {
        "id": "iTXB3CqRLQpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_config={\n",
        "    \"activation\":\"relu\",\n",
        "    \"batch_size\":64,\n",
        "    \"epochs\":10,\n",
        "    \"hidden_inputsize\":128,\n",
        "    \"learning_rate\":1e-03,\n",
        "    \"number_hidden\":4,\n",
        "    \"optimizer\":\"nadam\",\n",
        "    \"weight_decay\":0.0005,\n",
        "    \"weight_init\":\"xavier\"\n",
        "}\n",
        "\n",
        "wandb.init(config = best_config,project = \"CS6910-assignment1\")\n",
        "config = wandb.config\n",
        "obj = NeuralNetwork(x_train.shape[1], list(itertools.chain(*[[config.hidden_inputsize]*config.number_hidden, [10]])), config.number_hidden+1, config.activation, config.weight_init, config.weight_decay)\n",
        "obj.optimize(x_train, y_train, x_val, y_val, config.optimizer, config.learning_rate, config.epochs, config.batch_size)\n",
        "y_pred, test_accuracy = obj.test(x_test, y_test)\n"
      ],
      "metadata": {
        "id": "ovd2qM-e-GHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting confusion matrix"
      ],
      "metadata": {
        "id": "MjHgETCbONJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=y_pred.T, y_true=y_test, class_names=class_names)})"
      ],
      "metadata": {
        "id": "k5sscUK0OEmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8 : Comparing all the models on mean-squared loss and cross-entropy loss\n",
        "\n"
      ],
      "metadata": {
        "id": "yAt49hwG1Gtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'name':\"my-sweep\",\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "\n",
        "    'parameters': {\n",
        "        'learn_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'weight_initial': {\n",
        "            'values':['random','xavier']\n",
        "        },\n",
        "\n",
        "        'hidden_size': {\n",
        "            'values':[32, 64, 128]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['momentum_gradient_descent', 'nesterov', 'rmsprop', 'adam', 'nadam','stochastic_gradient_descent']\n",
        "        },\n",
        "        'batch_size' : {\n",
        "            'values':[16, 32, 64]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid','tanh','relu']\n",
        "        },\n",
        "        'hidden_layer': {\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "        'losscomputation':{\n",
        "            'values':['cross_entropy','mean_square']\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values':[0, 0.0005,  0.5]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [5, 10]\n",
        "        }\n",
        "\n",
        "        }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910-assignment1\")\n",
        "\n",
        "def train():\n",
        "  with wandb.init() as run:\n",
        "    config = wandb.config\n",
        "    wandb.run.name = \"hl_\" + str(config.hidden_size)+\"_bs_\"+str(config.batch_size)+\"_ac_\"+ config.activation+\"_lf_\"+str(config.losscomputation)\n",
        "    np.random.seed(1)\n",
        "    obj = NeuralNetwork(x_train.shape[1], list(itertools.chain(*[[config.hidden_inputsize]*config.number_hidden, [10]])), config.number_hidden+1, config.activation, config.weight_init, config.weight_decay)\n",
        "    obj.optimize(x_train, y_train, x_val, y_val, config.optimizer, config.learning_rate, config.epochs, config.batch_size)\n",
        "    y_pred, test_accuracy = obj.test(x_test, y_test)\n",
        "\n",
        "wandb.agent(sweep_id, train, project=\"CS6910-assignment1\")\n"
      ],
      "metadata": {
        "id": "_hG-7kF81CR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10 : Importing MNIST data to test best possible configurations. I have given 4 of them listed below int the sweep_config."
      ],
      "metadata": {
        "id": "oFi8SOYrxMql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "N2FITjwNw0ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'name':\"my-sweep-mnist\",\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'number_hidden': {\n",
        "            'values': [5]\n",
        "        },\n",
        "        'hidden_inputsize': {\n",
        "            'values':[64, 128]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values':[0]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adam', 'nadam']\n",
        "        },\n",
        "        'batch_size' : {\n",
        "            'values':[64]\n",
        "        },\n",
        "        'weight_init': {\n",
        "            'values':['xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu']\n",
        "        }\n",
        "\n",
        "        }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910-assignment1\")\n"
      ],
      "metadata": {
        "id": "EaMsV5jAw63-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    with wandb.init() as run:\n",
        "      config = wandb.config\n",
        "      wandb.run.name = \"hl_\" + str(config.hidden_inputsize)+\"_bs_\"+str(config.batch_size)+\"_ac_\"+ config.activation\n",
        "      np.random.seed(1)\n",
        "      obj = NeuralNetwork(x_train.shape[1], list(itertools.chain(*[[config.hidden_inputsize]*config.number_hidden, [10]])), config.number_hidden+1, config.activation, config.weight_init, config.weight_decay)\n",
        "      obj.optimize(x_train, y_train, x_val, y_val, config.optimizer, config.learning_rate, config.epochs, config.batch_size)\n",
        "      y_pred, test_accuracy = obj.test(x_test, y_test)\n",
        "\n",
        "wandb.agent(sweep_id,train,project=\"CS6910-assignment1\")\n"
      ],
      "metadata": {
        "id": "hwix5C05w9f2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}